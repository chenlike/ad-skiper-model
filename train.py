import os
import json
import argparse
import torch
import torchaudio
import dasheng
import random
import numpy as np
from torch import nn
from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler
from tqdm import tqdm
from sklearn.metrics import precision_score, recall_score, f1_score

# ----------------------------
# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
# ----------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed()

# ----------------------------
# è‡ªå®šä¹‰æ•°æ®é›†ï¼šæŒ‰å›ºå®šé•¿åº¦ segment_duration åˆ‡åˆ†éŸ³é¢‘ï¼Œ
# å¦‚æœä¸€ä¸ª segment éƒ¨åˆ†æ˜¯å¹¿å‘Šéƒ¨åˆ†æ˜¯éå¹¿å‘Šï¼Œåˆ™æŠ›å¼ƒã€‚
# æ¯ä¸ªæ ·æœ¬è¿”å› waveform, label, seg_start, total_duration
# ----------------------------
class AdSegmentDataset(Dataset):
    def __init__(self,
                 json_path: str,
                 audio_dir: str,
                 segment_duration: float = 5.0,
                 sample_rate: int = 16000,
                 max_items: int = -1):
        """
        json_path: æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„
        audio_dir: éŸ³é¢‘æ–‡ä»¶æ‰€åœ¨ç›®å½•
        segment_duration: æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5.0
        sample_rate: ç›®æ ‡é‡‡æ ·ç‡ï¼Œé»˜è®¤ 16000
        max_items: è‹¥ >0ï¼Œåˆ™åªåŠ è½½å‰ max_items æ¡è®°å½•ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
        """
        self.samples = []
        self.segment_duration = segment_duration
        self.sample_rate = sample_rate
        self.seg_len_samples = int(segment_duration * sample_rate)

        # è¯»å– JSON æ–‡ä»¶
        with open(json_path, 'r') as f:
            data = json.load(f)
        if max_items > 0:
            data = data[:max_items]

        # éå†æ¯æ¡æ ‡æ³¨
        for item in tqdm(data, desc="åŠ è½½æ•°æ®", unit="æ¡"):
            audio_path = os.path.join(audio_dir, item['audioPath'])
            if not os.path.exists(audio_path):
                continue  # æ–‡ä»¶ä¸å­˜åœ¨åˆ™è·³è¿‡

            # è¯»å–éŸ³é¢‘
            try:
                waveform, sr = torchaudio.load(audio_path)
            except Exception as e:
                print(f"Error loading {audio_path}: {e}")
                continue

            # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
            if sr != sample_rate:
                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)
                waveform = resampler(waveform)

            # è½¬ä¸ºå•å£°é“
            waveform = waveform.mean(dim=0, keepdim=True)  # [1, T_raw]
            total_duration = waveform.shape[1] / sample_rate  # ä»¥ç§’ä¸ºå•ä½

            # æå–å¹¿å‘ŠåŒºé—´ï¼Œå¹¶æŒ‰èµ·å§‹æ—¶é—´æ’åº
            ad_intervals = sorted([(ad['startTime'], ad['endTime']) for ad in item.get('ads', [])],
                                  key=lambda x: x[0])

            # åˆ¤æ–­æ•´ä¸ª [seg_start, seg_end] æ˜¯å¦å®Œå…¨åœ¨æŸä¸ªå¹¿å‘ŠåŒºé—´å†…
            def is_fully_in_ad(seg_start: float, seg_end: float, intervals: list):
                for a, b in intervals:
                    if seg_start >= a and seg_end <= b:
                        return True
                return False

            # åˆ¤æ–­ [seg_start, seg_end] æ˜¯å¦ä¸ä»»æ„å¹¿å‘ŠåŒºé—´æœ‰éƒ¨åˆ†é‡å 
            def has_overlap(seg_start: float, seg_end: float, intervals: list):
                for a, b in intervals:
                    if seg_start < b and seg_end > a:
                        return True
                return False

            # æŒ‰ segment_duration è¿›è¡Œéé‡å åˆ‡åˆ†
            step = segment_duration
            current = 0.0
            while current + segment_duration <= total_duration + 1e-6:
                seg_start = current
                seg_end = current + segment_duration

                # æ£€æŸ¥æ˜¯å¦ä¸å¹¿å‘ŠåŒºé—´é‡å 
                overlap = has_overlap(seg_start, seg_end, ad_intervals)
                if overlap:
                    # å¦‚æœå®Œå…¨åŒ…å«åœ¨æŸä¸ªå¹¿å‘ŠåŒºé—´å†…ï¼Œåˆ’ä¸ºå¹¿å‘Š
                    if is_fully_in_ad(seg_start, seg_end, ad_intervals):
                        start_sample = int(seg_start * sample_rate)
                        end_sample = start_sample + self.seg_len_samples
                        if end_sample <= waveform.shape[1]:
                            segment = waveform[:, start_sample:end_sample]
                            if segment.shape[1] == self.seg_len_samples:
                                self.samples.append((segment, 1, seg_start, total_duration))
                    # éƒ¨åˆ†é‡å åˆ™ä¸¢å¼ƒ
                    # else: do nothing
                else:
                    # å®Œå…¨ä¸ä¸ä»»ä½•å¹¿å‘ŠåŒºé—´é‡å ï¼Œæ ‡ä¸ºéå¹¿å‘Š
                    start_sample = int(seg_start * sample_rate)
                    end_sample = start_sample + self.seg_len_samples
                    if end_sample <= waveform.shape[1]:
                        segment = waveform[:, start_sample:end_sample]
                        if segment.shape[1] == self.seg_len_samples:
                            self.samples.append((segment, 0, seg_start, total_duration))

                current += step

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx: int):
        waveform, label, seg_start, total_duration = self.samples[idx]
        return (
            waveform,
            torch.tensor(label, dtype=torch.float32),
            torch.tensor(seg_start, dtype=torch.float32),
            torch.tensor(total_duration, dtype=torch.float32)
        )


# ----------------------------
# ç”ŸæˆåŸºäºâ€œæ’­æ”¾æ¯”ä¾‹â€ çš„æ­£ä½™å¼¦ä½ç½®ç¼–ç 
# ----------------------------
def get_ratio_positional_encoding(num_steps: int, d_model: int, segment_duration: float, seg_start: torch.Tensor, total_duration: torch.Tensor, device):
    """
    num_steps: åºåˆ—é•¿åº¦ T'
    d_model: ç‰¹å¾ç»´åº¦ D
    segment_duration: ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œè¿™é‡Œå›ºå®š 5.0
    seg_start: Tensor [B]ï¼Œè¯¥ç‰‡æ®µåœ¨åŸéŸ³é¢‘çš„èµ·å§‹ç§’æ•°
    total_duration: Tensor [B]ï¼Œè¯¥éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
    device: è®¾å¤‡
    è¿”å›: [B, T', D] çš„ä½ç½®ç¼–ç 
    """
    B = seg_start.size(0)
    T_prime = num_steps

    # è®¡ç®—æ¯å¸§å¯¹åº”çš„çœŸå®æ—¶é—´ç™¾åˆ†æ¯”
    ratio_start = seg_start / (total_duration + 1e-6)  # é˜²æ­¢é™¤é›¶

    # æ¯å¸§å¯¹åº”çš„ç§’æ•°æ­¥é•¿
    frame_sec = segment_duration / T_prime  # æ ‡é‡

    # æ„é€  idx = [0,1,...,T'-1]
    idx = torch.arange(0, T_prime, dtype=torch.float32, device=device)  # [T']
    idx_frame = idx.unsqueeze(0).expand(B, T_prime)  # [B, T']

    # æ‰©å±• ratio_start ä¸ total_duration åˆ° [B, T']
    ratio_start_expand = ratio_start.unsqueeze(1).expand(B, T_prime)       # [B, T']
    total_d_expand = total_duration.unsqueeze(1).expand(B, T_prime)        # [B, T']

    # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å…¨å±€ç™¾åˆ†æ¯”ä½ç½®ï¼š
    pe_ratio = ratio_start_expand + idx_frame * (frame_sec / total_d_expand)  # [B, T']

    # ä¸º pe_ratio ç¼–ç æˆ [B, T', D] çš„æ­£ä½™å¼¦
    pe = torch.zeros((B, T_prime, d_model), device=device)
    k = torch.arange(0, d_model // 2, device=device, dtype=torch.float32)
    denom = torch.pow(10000, (2 * k) / d_model)  # [D/2]
    angle = pe_ratio.unsqueeze(2) / denom.unsqueeze(0).unsqueeze(0)  # [B, T', D/2]

    pe[:, :, 0::2] = torch.sin(angle)
    pe[:, :, 1::2] = torch.cos(angle)
    return pe  # [B, T', D]


# ----------------------------
# æ¨¡å‹å®šä¹‰ï¼šåœ¨ dasheng åŸºç¡€ä¸ŠåŠ ä¸€å±‚ Transformer æ³¨æ„åŠ›å±‚ï¼Œä½ç½®ç¼–ç ä½¿ç”¨â€œæ’­æ”¾æ¯”ä¾‹â€
# ----------------------------
class DashengAdClassifier(nn.Module):
    def __init__(self,
                 freeze_dasheng: bool = True,
                 nhead: int = 8,
                 dim_feedforward: int = None,
                 num_transformer_layers: int = 1,
                 segment_duration: float = 5.0
                 ):
        super().__init__()
        # ä½¿ç”¨ dasheng ä½œä¸º backbone
        self.dashengmodel = dasheng.dasheng_12B()
        self.freeze_dasheng = freeze_dasheng

        if self.freeze_dasheng:
            print("ğŸš« å†»ç»“ dasheng éª¨å¹²æ‰€æœ‰å‚æ•°")
            for param in self.dashengmodel.parameters():
                param.requires_grad = False

        D = self.dashengmodel.embed_dim
        if dim_feedforward is None:
            dim_feedforward = D * 4

        encoder_layer = nn.TransformerEncoderLayer(d_model=D,
                                                   nhead=nhead,
                                                   dim_feedforward=dim_feedforward,
                                                   batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)

        self.classifier = nn.Sequential(
            nn.LayerNorm(D),
            nn.Linear(D, 1)
        )

        self.embed_dim = D
        self.segment_duration = segment_duration 

    def forward(self, x: torch.Tensor, seg_start: torch.Tensor, total_duration: torch.Tensor):
        """
        x: [B, 1, seg_len_samples]
        seg_start: [B]ï¼Œè¯¥ç‰‡æ®µåœ¨åŸéŸ³é¢‘ä¸­çš„èµ·å§‹ç§’æ•°
        total_duration: [B]ï¼Œè¯¥éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        B = x.size(0)
        device = x.device

        # å…ˆå»æ‰é¢‘é“ç»´åº¦ -> [B, seg_len_samples]
        x = x.squeeze(1)

        # 1) dasheng æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡º [B, T', D]
        with torch.set_grad_enabled(not self.freeze_dasheng):
            seq_feats = self.dashengmodel(x)  # [B, T', D]

        T_prime = seq_feats.size(1)
        D = self.embed_dim

        # 2) ç”Ÿæˆâ€œæ’­æ”¾æ¯”ä¾‹â€ä½ç½®ç¼–ç å¹¶åŠ åˆ° seq_feats
        pe = get_ratio_positional_encoding(num_steps=T_prime,
                                           d_model=D,
                                           segment_duration=self.segment_duration,
                                           seg_start=seg_start,
                                           total_duration=total_duration,
                                           device=device)  # [B, T', D]
        seq_feats = seq_feats + pe

        # 3) Transformer Encoder
        transformer_out = self.transformer_encoder(seq_feats)  # [B, T', D]

        # 4) å¯¹æ—¶é—´ç»´åº¦åšå¹³å‡ Pooling -> [B, D]
        pooled = transformer_out.mean(dim=1)  # [B, D]

        # 5) åˆ†ç±»å¹¶è¿”å› logit [B]
        logits = self.classifier(pooled).squeeze(-1)  # [B]
        return logits


# ----------------------------
# è®­ç»ƒä¸€ä¸ª epoch
# ----------------------------
def train_one_epoch(model: nn.Module,
                    dataloader: DataLoader,
                    criterion: nn.Module,
                    optimizer: torch.optim.Optimizer,
                    device: torch.device) -> float:
    model.train()
    total_loss = 0.0
    bar = tqdm(dataloader, desc="Training", leave=False)
    for waveform, labels, seg_start, total_duration in bar:
        waveform = waveform.to(device)         # [B, 1, seg_len_samples]
        labels = labels.to(device)             # [B]
        seg_start = seg_start.to(device)       # [B]
        total_duration = total_duration.to(device)  # [B]

        optimizer.zero_grad()
        logits = model(waveform, seg_start, total_duration)  # [B]
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * waveform.size(0)
        bar.set_postfix(loss=loss.item())
    return total_loss / len(dataloader.dataset)


# ----------------------------
# éªŒè¯å‡½æ•°ï¼ˆå¸¦å‡†ç¡®ç‡ã€Precisionã€Recallã€F1ï¼‰
# ----------------------------
def evaluate(model: nn.Module,
             dataloader: DataLoader,
             criterion: nn.Module,
             device: torch.device):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    correct = 0

    bar = tqdm(dataloader, desc="Evaluating", leave=False)
    with torch.no_grad():
        for waveform, labels, seg_start, total_duration in bar:
            waveform = waveform.to(device)         # [B, 1, seg_len_samples]
            labels = labels.to(device)             # [B]
            seg_start = seg_start.to(device)       # [B]
            total_duration = total_duration.to(device)  # [B]

            logits = model(waveform, seg_start, total_duration)  # [B]
            loss = criterion(logits, labels)
            total_loss += loss.item() * waveform.size(0)

            probs = torch.sigmoid(logits)
            preds = (probs >= 0.5).float()
            correct += (preds == labels).sum().item()

            all_preds.extend(preds.cpu().numpy().tolist())
            all_labels.extend(labels.cpu().numpy().tolist())

    accuracy = correct / len(dataloader.dataset)
    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    return total_loss / len(dataloader.dataset), accuracy, precision, recall, f1


# ----------------------------
# ä¸»å‡½æ•°
# ----------------------------
def main():
    # ----------------------------
    # é€šè¿‡ argparse æ·»åŠ â€œç»­è®­â€å‚æ•°
    # ----------------------------
    parser = argparse.ArgumentParser(description="å¹¿å‘Šç‰‡æ®µåˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument('--json_path', type=str, default='./audio_ads.json', help='æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, default='./audio/', help='éŸ³é¢‘ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=5, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--val_ratio', type=float, default=0.15, help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--freeze_dasheng', action='store_true', help='æ˜¯å¦å†»ç»“ dasheng éª¨å¹²')
    parser.add_argument('--max_items', type=int, default=-1, help='æœ€å¤§æ ·æœ¬æ•°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--segment_duration', type=float, default=4.0, help='æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--resume_pth', type=str, default=None, help='å·²è®­ç»ƒå¥½çš„ .pth æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºç»§ç»­è®­ç»ƒ')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ----------------------------
    # åŠ è½½æ•°æ®é›†
    # ----------------------------
    dataset = AdSegmentDataset(json_path=args.json_path,
                               audio_dir=args.audio_dir,
                               segment_duration=args.segment_duration,
                               sample_rate=16000,
                               max_items=args.max_items)  # max_items å¯è®¾ 50 è¿›è¡Œå¿«é€Ÿæµ‹è¯•

    # æå–æ‰€æœ‰åˆ‡ç‰‡çš„æ ‡ç­¾ï¼Œç”¨äºåˆ’åˆ†
    labels_all = np.array([label for _, label, _, _ in dataset.samples])
    ad_indices = np.where(labels_all == 1)[0]
    non_ad_indices = np.where(labels_all == 0)[0]

    # æ‰“ä¹±ç´¢å¼•
    np.random.shuffle(ad_indices)
    np.random.shuffle(non_ad_indices)

    # åˆ’åˆ†éªŒè¯é›†æ•°é‡
    ad_val_size = int(len(ad_indices) * args.val_ratio)
    non_ad_val_size = int(len(non_ad_indices) * args.val_ratio)

    ad_val_idx = ad_indices[:ad_val_size]
    ad_train_idx = ad_indices[ad_val_size:]
    non_ad_val_idx = non_ad_indices[:non_ad_val_size]
    non_ad_train_idx = non_ad_indices[non_ad_val_size:]

    # ä¸‹é‡‡æ ·éå¹¿å‘Šè®­ç»ƒæ ·æœ¬ï¼ˆåªä¿ç•™ 50%ï¼‰
    non_ad_keep_ratio = 0.5
    keep_non_ad_train_idx = non_ad_train_idx[:int(len(non_ad_train_idx) * non_ad_keep_ratio)]

    # åˆå¹¶è®­ç»ƒ/éªŒè¯ç´¢å¼•
    train_idx = np.concatenate([ad_train_idx, keep_non_ad_train_idx])
    val_idx = np.concatenate([ad_val_idx, non_ad_val_idx])
    np.random.shuffle(train_idx)
    np.random.shuffle(val_idx)

    train_dataset = Subset(dataset, train_idx)
    val_dataset = Subset(dataset, val_idx)

    # ----------------------------
    # è®¡ç®—è®­ç»ƒé›†æ­£è´Ÿæ ·æœ¬æƒé‡ï¼Œç”¨äºåŠ æƒ Loss å’Œé‡‡æ ·
    # ----------------------------
    train_labels = np.array([dataset.samples[i][1] for i in train_idx])
    num_pos = int(train_labels.sum())
    num_neg = len(train_labels) - num_pos

    pos_weight = torch.tensor([num_neg / (num_pos + 1e-6)], dtype=torch.float32).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    weight_pos = 1.0 / (num_pos + 1e-6)
    weight_neg = 1.0 / (num_neg + 1e-6)
    sample_weights = [weight_pos if label == 1 else weight_neg for label in train_labels]
    sampler = WeightedRandomSampler(sample_weights,
                                    num_samples=len(sample_weights),
                                    replacement=True)

    # ----------------------------
    # åˆ›å»º DataLoader
    # ----------------------------
    train_loader = DataLoader(train_dataset,
                              batch_size=args.batch_size,
                              sampler=sampler,
                              num_workers=2,
                              pin_memory=True)
    val_loader = DataLoader(val_dataset,
                            batch_size=args.batch_size,
                            shuffle=False,
                            num_workers=2,
                            pin_memory=True)

    # ----------------------------
    # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨
    # ----------------------------
    model = DashengAdClassifier(freeze_dasheng=args.freeze_dasheng,
                                nhead=8,
                                dim_feedforward=None,
                                num_transformer_layers=1,
                                segment_duration=args.segment_duration).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    # å¦‚æœç”¨æˆ·æŒ‡å®šäº† --resume_pthï¼Œå°±åŠ è½½å·²æœ‰çš„æ¨¡å‹æƒé‡
    start_epoch = 0
    best_f1 = 0.0
    if args.resume_pth is not None and os.path.isfile(args.resume_pth):
        print(f"ğŸ”„ ä»å·²æœ‰æ¨¡å‹ {args.resume_pth} åŠ è½½æƒé‡ï¼Œå¹¶ç»§ç»­è®­ç»ƒ")
        checkpoint = torch.load(args.resume_pth, map_location=device)
        # å¦‚æœä¿å­˜çš„æ˜¯ state_dictï¼Œåˆ™ç›´æ¥åŠ è½½
        model.load_state_dict(checkpoint)
        # å¦‚æœæƒ³æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œéœ€è¦åœ¨ä¿å­˜æ—¶ä¸€å¹¶ä¿å­˜ optimizer.state_dict()
        # ä¾‹å¦‚ï¼štorch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}, 'xxx.pth')
        # ç„¶åè¿™é‡ŒåŠ è½½å¹¶æ¢å¤ï¼šoptimizer.load_state_dict(checkpoint['optimizer']); start_epoch = checkpoint['epoch'] + 1
        # è‹¥ checkpoint åªæ˜¯æ¨¡å‹æƒé‡ï¼Œåˆ™ start_epoch ä¾ç„¶ä» 0 å¼€å§‹
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ\n")
    else:
        if args.resume_pth is not None:
            print(f"âš ï¸ æŒ‡å®šçš„ resume_pth æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.resume_pth}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ\n")

    # ----------------------------
    # è®­ç»ƒä¸éªŒè¯å¾ªç¯
    # ----------------------------
    best_model_path = 'best_model.pth'
    for epoch in range(start_epoch, args.num_epochs):
        print(f"\n======== Epoch {epoch+1}/{args.num_epochs} ========")
        # è®­ç»ƒ
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        print(f"Train Loss: {train_loss:.4f}")

        # éªŒè¯
        val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)
        print(f"Val   Loss: {val_loss:.4f} | Acc: {accuracy:.4f} | "
              f"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆä»¥ F1 ä¸ºå‡†ï¼‰
        if f1 > best_f1:
            best_f1 = f1
            torch.save(model.state_dict(), best_model_path)
            print(f"âœ¨ ä¿å­˜æ–°æœ€ä½³æ¨¡å‹ï¼ŒF1 æå‡åˆ° {best_f1:.4f}")

    print(f"\nè®­ç»ƒç»“æŸï¼Œæœ€ä½³ F1 ä¸º {best_f1:.4f}")
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")


if __name__ == '__main__':
    main()

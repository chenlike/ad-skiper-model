import os
import json
import argparse
import subprocess
import tempfile

import torch
import torchaudio
import dasheng
import numpy as np
from torch import nn

# ===================================================================
# æ¨¡å‹ç±»å®šä¹‰ (ä¸ä¹‹å‰ç›¸åŒ)
# ===================================================================
class ContextualAdClassifier(nn.Module):
    def __init__(self, backbone, freeze_backbone=True, d_model=768, nhead=8, num_layers=3):
        super().__init__()
        self.backbone = backbone
        self.freeze_backbone = freeze_backbone

        if self.freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        self.d_model = self.backbone.embed_dim

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, 
            nhead=nhead, 
            dim_feedforward=self.d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, 1)
        )

    def forward(self, x: torch.Tensor):
        B, S, C, L = x.shape
        x = x.view(B * S, C, L).squeeze(1)
        
        with torch.set_grad_enabled(not self.freeze_backbone):
            sequence_features = self.backbone(x) 
            segment_embeddings = sequence_features.mean(dim=1)

        segment_embeddings = segment_embeddings.view(B, S, -1)
        contextual_embeddings = self.transformer_encoder(segment_embeddings)
        logits = self.classifier(contextual_embeddings).squeeze(-1)
        probs = torch.sigmoid(logits)
        return probs

# ===================================================================
# æ¨ç†ä¸»å‡½æ•°
# ===================================================================
def run_inference(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"======= ä½¿ç”¨è®¾å¤‡: {device} =======")

    # 1. æ¨¡å‹åŠ è½½ (ä¸ä¹‹å‰ç›¸åŒ)
    print("ğŸ› ï¸  åŠ è½½æ¨¡å‹...")
    backbone = dasheng.dasheng_base()
    model = ContextualAdClassifier(backbone=backbone, freeze_backbone=False, num_layers=3, nhead=8).to(device)
    model.load_state_dict(torch.load(args.model_path, map_location=device))
    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model_path}")

    # 2. éŸ³é¢‘åŠ è½½ä¸è½¬æ¢ (ä¸ä¹‹å‰ç›¸åŒ)
    audio_path_to_load = args.audio_path
    temp_wav_path = None
    if audio_path_to_load.lower().endswith('.mp3'):
        tmp_f = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        temp_wav_path = tmp_f.name
        tmp_f.close()
        print(f"ğŸï¸  æ£€æµ‹åˆ° MP3ï¼Œæ­£åœ¨è°ƒç”¨ ffmpeg è½¬ä¸ºä¸´æ—¶ WAV...")
        ffmpeg_cmd = ['ffmpeg', '-y', '-i', audio_path_to_load, '-ar', str(args.sample_rate), '-ac', '1', temp_wav_path]
        try:
            subprocess.run(ffmpeg_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError as e:
            print(f"âŒ é”™è¯¯ï¼šffmpeg è½¬ç å¤±è´¥ã€‚è¯·ç¡®ä¿å·²å®‰è£… ffmpeg å¹¶å°†å…¶æ·»åŠ è‡³ç³»ç»Ÿè·¯å¾„ã€‚")
            if os.path.exists(temp_wav_path): os.remove(temp_wav_path)
            return
        audio_path_to_load = temp_wav_path
    print(f"ğŸ§ æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶: {args.audio_path} ...")
    try:
        waveform, sr = torchaudio.load(audio_path_to_load)
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶. {e}")
        if temp_wav_path and os.path.exists(temp_wav_path): os.remove(temp_wav_path)
        return
    if temp_wav_path and os.path.exists(temp_wav_path):
        os.remove(temp_wav_path)

    # 3. é¢„å¤„ç† (ä¸ä¹‹å‰ç›¸åŒ)
    if sr != args.sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=args.sample_rate)
        waveform = resampler(waveform)
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # 4 & 5. åˆ‡ç‰‡ä¸æ‰“åŒ…åºåˆ— (ä¸ä¹‹å‰ç›¸åŒ)
    seg_len_samples = int(args.segment_duration * args.sample_rate)
    step_samples = seg_len_samples // 2
    total_samples = waveform.shape[1]
    all_segments = [waveform[:, s:s + seg_len_samples] for s in range(0, total_samples - seg_len_samples + 1, step_samples)]
    if not all_segments:
        print("âŒ éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•åˆ‡åˆ†å‡ºä»»ä½•ç‰‡æ®µã€‚")
        return
    all_sequences = []
    if len(all_segments) >= args.sequence_length:
        for i in range(len(all_segments) - args.sequence_length + 1):
            all_sequences.append(torch.stack(all_segments[i:i + args.sequence_length]))
    else:
        print(f"âš ï¸ è­¦å‘Š: éŸ³é¢‘ç‰‡æ®µæ•°ä¸è¶³ {args.sequence_length}ï¼Œæ— æ³•è¿›è¡Œæ¨ç†ã€‚")
        return
        
    # 6. æ¨¡å‹æ¨ç† (ä¸ä¹‹å‰ç›¸åŒ)
    print(f"ğŸš€ å¼€å§‹æ¨ç†...")
    segment_predictions = np.zeros(len(all_segments))
    segment_counts = np.zeros(len(all_segments))
    with torch.no_grad():
        for i in range(0, len(all_sequences), args.batch_size):
            batch_tensor = torch.stack(all_sequences[i:i + args.batch_size]).to(device)
            probs = model(batch_tensor).cpu().numpy()
            for j, seq_probs in enumerate(probs):
                original_sequence_index = i + j
                for k, prob in enumerate(seq_probs):
                    original_segment_index = original_sequence_index + k
                    segment_predictions[original_segment_index] += prob
                    segment_counts[original_segment_index] += 1
    segment_counts[segment_counts == 0] = 1
    final_segment_probs = segment_predictions / segment_counts

    # è¾“å‡ºæ¯ä¸ªç‰‡æ®µçš„æ—¶é—´å’Œæ¦‚ç‡
    print("ğŸ“Š ç‰‡æ®µæ£€æµ‹ç»“æœè¯¦æƒ…:")
    print(f"{'åºå·':<4} {'å¼€å§‹æ—¶é—´':<12} {'ç»“æŸæ—¶é—´':<12} {'æ¦‚ç‡':<8} {'åˆ¤æ–­'}")
    print("-" * 50)
    for i, prob in enumerate(final_segment_probs):
        segment_start_sec = (i * step_samples) / args.sample_rate
        segment_end_sec = segment_start_sec + args.segment_duration
        is_ad = "å¹¿å‘Š" if prob > args.threshold else "éå¹¿å‘Š"
        print(f"{i+1:<4} {format_time(segment_start_sec):<12} {format_time(segment_end_sec):<12} {prob:8.4f} {is_ad}")
    print("-" * 50)

    # 7. åå¤„ç†
    print("ğŸ“ˆ åå¤„ç†ç»“æœ...")
    final_segment_labels = (final_segment_probs > args.threshold).astype(int)
    ad_timestamps = []
    is_in_ad = False
    ad_start_time = 0
    for i, label in enumerate(final_segment_labels):
        segment_start_sec = (i * step_samples) / args.sample_rate
        if label == 1 and not is_in_ad:
            is_in_ad = True
            ad_start_time = segment_start_sec
        elif label == 0 and is_in_ad:
            is_in_ad = False
            ad_end_time = segment_start_sec + (args.segment_duration / 2)
            ad_timestamps.append({"startTime": round(ad_start_time, 2), "endTime": round(ad_end_time, 2)})
    if is_in_ad:
        last_segment_start = ((len(final_segment_labels) - 1) * step_samples) / args.sample_rate
        ad_end_time = last_segment_start + args.segment_duration
        ad_timestamps.append({"startTime": round(ad_start_time, 2), "endTime": round(ad_end_time, 2)})

    # --- å…³é”®åå¤„ç†æ­¥éª¤ ---

    # æ­¥éª¤ 7a: é¦–å…ˆï¼Œæ ¹æ®æœ€å°å¹¿å‘Šæ—¶é•¿ï¼Œè¿‡æ»¤æ‰æ‰€æœ‰ä¸åˆæ ¼çš„çŸ­å¹¿å‘Š
    if args.min_ad_duration > 0 and ad_timestamps:
        print(f"ğŸ—‘ï¸  æ­¥éª¤1: è¿‡æ»¤æ‰æ—¶é•¿å°äº {args.min_ad_duration}s çš„å¹¿å‘Š...")
        original_count = len(ad_timestamps)
        ad_timestamps = [ad for ad in ad_timestamps if (ad['endTime'] - ad['startTime']) >= args.min_ad_duration]
        print(f"   - è¿‡æ»¤å‰: {original_count} æ¡, è¿‡æ»¤å: {len(ad_timestamps)} æ¡")

    # æ­¥éª¤ 7b: ç„¶åï¼Œåœ¨è¿‡æ»¤åçš„åˆæ ¼å¹¿å‘Šåˆ—è¡¨åŸºç¡€ä¸Šï¼Œåˆå¹¶é—´è·è¾ƒè¿‘çš„å¹¿å‘Š
    if args.merge_gap_duration > 0 and len(ad_timestamps) > 1:
        print(f"ğŸ”„ æ­¥éª¤2: åœ¨åˆæ ¼å¹¿å‘Šä¹‹é—´ï¼Œåˆå¹¶å°äº {args.merge_gap_duration}s çš„é—´è·...")
        original_count = len(ad_timestamps)
        merged_ads = [ad_timestamps[0]]
        for current_ad in ad_timestamps[1:]:
            last_merged_ad = merged_ads[-1]
            gap = current_ad['startTime'] - last_merged_ad['endTime']
            if gap <= args.merge_gap_duration:
                last_merged_ad['endTime'] = current_ad['endTime']
            else:
                merged_ads.append(current_ad)
        ad_timestamps = merged_ads
        print(f"   - åˆå¹¶å‰: {original_count} æ¡, åˆå¹¶å: {len(ad_timestamps)} æ¡")

    # 8. ä¿å­˜ç»“æœ
    segment_results = []
    for i, prob in enumerate(final_segment_probs):
        segment_start_sec = (i * step_samples) / args.sample_rate
        segment_end_sec = segment_start_sec + args.segment_duration
        segment_results.append({
            "startTime": round(segment_start_sec, 2),
            "endTime": round(segment_end_sec, 2),
            "probability": round(float(prob), 4),
            "isAd": bool(prob > args.threshold)
        })

    output_data = {
        "audioPath": os.path.basename(args.audio_path),
        "detectedAds": ad_timestamps,
        "segmentResults": segment_results
    }
    with open(args.output_json, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
        
    print(f"âœ… æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {args.output_json}")
    if ad_timestamps:
        print("æ£€æµ‹åˆ°çš„æœ€ç»ˆå¹¿å‘Šæ—¶é—´æˆ³:")
        for ad in ad_timestamps:
            duration = ad['endTime'] - ad['startTime']
            print(f"  - å¼€å§‹: {format_time(ad['startTime'])}, "
                  f"ç»“æŸ: {format_time(ad['endTime'])} "
                  f"(æ—¶é•¿: {format_time(duration)})")
    else:
        print("æœªæ£€æµ‹åˆ°æ»¡è¶³æ¡ä»¶çš„å¹¿å‘Šã€‚")

def format_time(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸º 'MM:SS.xx (seconds)' æ ¼å¼çš„å­—ç¬¦ä¸²
    
    Args:
        seconds (float): éœ€è¦è½¬æ¢çš„ç§’æ•°
    
    Returns:
        str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²
    """
    minutes = int(seconds) // 60
    secs = seconds % 60
    return f"{minutes:02d}:{secs:05.2f} ({seconds:.2f}s)"

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šæ£€æµ‹æ¨¡å‹æ¨ç†è„šæœ¬",
        formatter_class=argparse.RawTextHelpFormatter
    )
    # å‚æ•°å®šä¹‰...
    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ (.pth)')
    parser.add_argument('--audio_path', type=str, required=True, help='éœ€è¦æ£€æµ‹çš„å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„ (æ”¯æŒ .wav, .mp3)')
    parser.add_argument('--output_json', type=str, default='detected_ads.json', help='è¾“å‡ºç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--segment_duration', type=float, default=3.0, help='æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--sequence_length', type=int, default=8, help='Transformer çš„ä¸Šä¸‹æ–‡çª—å£å¤§å° (ç‰‡æ®µæ•°é‡)')
    parser.add_argument('--sample_rate', type=int, default=16000, help='éŸ³é¢‘é‡‡æ ·ç‡')
    parser.add_argument('--batch_size', type=int, default=16, help='æ¨ç†æ—¶ä½¿ç”¨çš„æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--threshold', type=float, default=0.5, help='åˆ¤æ–­ä¸ºå¹¿å‘Šçš„æ¦‚ç‡é˜ˆå€¼')
    
    # --- åå¤„ç†å‚æ•° (é™„å¸¦æ›´æ¸…æ™°çš„è¯´æ˜) ---
    parser.add_argument(
        '--min_ad_duration', 
        type=float, 
        default=0, 
        help='è¿‡æ»¤æ‰æ—¶é•¿å°äºæ­¤å€¼(ç§’)çš„å¹¿å‘Šã€‚\n'
             'ä¾‹å¦‚ï¼Œè®¾ä¸º5ï¼Œåˆ™æ‰€æœ‰æ—¶é•¿å°äº5ç§’çš„å¹¿å‘Šä¼šè¢«é¦–å…ˆä¸¢å¼ƒã€‚\n'
             'é»˜è®¤ä¸º0ï¼Œä¸è¿‡æ»¤ã€‚'
    )
    parser.add_argument(
        '--merge_gap_duration', 
        type=float, 
        default=0, 
        help='åœ¨æ»¡è¶³min_ad_durationçš„å¹¿å‘Šä¹‹é—´ï¼Œ\n'
             'å¦‚æœéå¹¿å‘Šé—´è·å°äºæ­¤å€¼(ç§’)ï¼Œåˆ™å°†å®ƒä»¬åˆå¹¶ã€‚\n'
             'ä¾‹å¦‚ï¼Œè®¾ä¸º60ï¼Œ[åˆæ ¼å¹¿å‘ŠA][60ç§’éå¹¿å‘Š][åˆæ ¼å¹¿å‘ŠB]ä¼šè¢«åˆå¹¶ã€‚\n'
             'é»˜è®¤ä¸º0ï¼Œä¸åˆå¹¶ã€‚'
    )
    
    args = parser.parse_args()
    run_inference(args)
import os
import json
import argparse
import torch
import torchaudio
import dasheng
import numpy as np
from torch import nn

# ===================================================================
# é‡è¦ï¼šä½ éœ€è¦ä»ä½ çš„è®­ç»ƒè„šæœ¬ä¸­å¤åˆ¶æ¨¡å‹ç±»çš„å®šä¹‰
# Pythonéœ€è¦çŸ¥é“ç±»çš„ç»“æ„æ‰èƒ½åŠ è½½æ¨¡å‹æƒé‡
# ===================================================================
class ContextualAdClassifier(nn.Module):
    def __init__(self, backbone, freeze_backbone=True, d_model=768, nhead=8, num_layers=3):
        super().__init__()
        self.backbone = backbone
        self.freeze_backbone = freeze_backbone

        if self.freeze_backbone:
            # åœ¨æ¨ç†æ—¶ï¼Œå†»ç»“ä¸å¦ä¸å½±å“ç»“æœï¼Œä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§è€Œä¿ç•™
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        self.d_model = self.backbone.embed_dim

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, 
            nhead=nhead, 
            dim_feedforward=self.d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, 1)
        )

    def forward(self, x: torch.Tensor):
        B, S, C, L = x.shape
        x = x.view(B * S, C, L).squeeze(1)
        
        with torch.set_grad_enabled(not self.freeze_backbone):
            sequence_features = self.backbone(x) 
            segment_embeddings = sequence_features.mean(dim=1)

        segment_embeddings = segment_embeddings.view(B, S, -1)
        contextual_embeddings = self.transformer_encoder(segment_embeddings)
        logits = self.classifier(contextual_embeddings).squeeze(-1)
        probs = torch.sigmoid(logits)
        
        return probs

# ===================================================================
# æ¨ç†ä¸»å‡½æ•°
# ===================================================================
def run_inference(args):
    """
    å¯¹å•ä¸ªéŸ³é¢‘æ–‡ä»¶è¿›è¡Œå¹¿å‘Šæ£€æµ‹æ¨ç†ã€‚
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"======= ä½¿ç”¨è®¾å¤‡: {device} =======")

    # 1. åˆå§‹åŒ–æ¨¡å‹ç»“æ„å¹¶åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    print("ğŸ› ï¸  åŠ è½½æ¨¡å‹...")
    backbone = dasheng.dasheng_base()
    model = ContextualAdClassifier(
        backbone=backbone,
        freeze_backbone=True, # æ¨ç†æ—¶æ­¤å‚æ•°ä¸é‡è¦
        num_layers=3, # ç¡®ä¿è¿™äº›è¶…å‚æ•°ä¸ä½ è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´
        nhead=8
    ).to(device)
    
    # åŠ è½½çŠ¶æ€å­—å…¸
    model.load_state_dict(torch.load(args.model_path, map_location=device))
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆè¿™ä¼šå…³é—­dropoutç­‰å±‚ï¼‰
    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model_path}")

    # 2. åŠ è½½å¹¶é¢„å¤„ç†éŸ³é¢‘ (ä¸è®­ç»ƒæ—¶çš„Dataseté€»è¾‘ä¸€è‡´)
    print(f"ğŸ§ æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶: {args.audio_path}...")
    try:
        waveform, sr = torchaudio.load(args.audio_path)
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶. {e}")
        return

    # é‡é‡‡æ ·å’Œè½¬å•å£°é“
    if sr != args.sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=args.sample_rate)
        waveform = resampler(waveform)
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
    
    # 3. å°†æ•´ä¸ªéŸ³é¢‘åˆ‡åˆ†ä¸ºé‡å çš„ç‰‡æ®µ (Segments)
    seg_len_samples = int(args.segment_duration * args.sample_rate)
    step_samples = seg_len_samples // 2 # 50% é‡å 
    total_samples = waveform.shape[1]
    
    all_segments = []
    for start_sample in range(0, total_samples - seg_len_samples + 1, step_samples):
        end_sample = start_sample + seg_len_samples
        segment_waveform = waveform[:, start_sample:end_sample]
        all_segments.append(segment_waveform)

    if not all_segments:
        print("âŒ éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•åˆ‡åˆ†å‡ºä»»ä½•ç‰‡æ®µã€‚")
        return

    # 4. å°†ç‰‡æ®µæ‰“åŒ…æˆåºåˆ— (Sequences)ï¼Œä¸æ¨¡å‹è¾“å…¥åŒ¹é…
    all_sequences = []
    if len(all_segments) >= args.sequence_length:
        for i in range(len(all_segments) - args.sequence_length + 1):
            sequence = all_segments[i:i + args.sequence_length]
            # å †å æˆ [Seq_Len, 1, Samples]
            all_sequences.append(torch.stack(sequence))
    else:
        print(f"âš ï¸ è­¦å‘Š: éŸ³é¢‘ç‰‡æ®µæ•° ({len(all_segments)}) å°äºåºåˆ—é•¿åº¦ ({args.sequence_length}). æ— æ³•è¿›è¡Œæ¨ç†ã€‚")
        return
        
    # 5. æ¨¡å‹æ¨ç†
    print(f"ğŸš€ å¼€å§‹æ¨ç†... å…± {len(all_sequences)} ä¸ªåºåˆ—")
    
    # ç”¨äºå­˜å‚¨æ¯ä¸ªç‹¬ç«‹ç‰‡æ®µçš„é¢„æµ‹ç»“æœï¼ˆå¤„ç†é‡å é—®é¢˜ï¼‰
    # æ•°ç»„çš„é•¿åº¦æ˜¯æ€»ç‰‡æ®µæ•°
    segment_predictions = np.zeros(len(all_segments))
    segment_counts = np.zeros(len(all_segments))

    with torch.no_grad():
        # ä¸ºäº†é˜²æ­¢æ˜¾å­˜æº¢å‡ºï¼Œå¯ä»¥åˆ†æ‰¹å¤„ç†åºåˆ—
        for i in range(0, len(all_sequences), args.batch_size):
            batch_sequences_list = all_sequences[i:i + args.batch_size]
            
            # æ•´ç†æˆä¸€ä¸ªæ‰¹æ¬¡ [B, Seq_Len, 1, Samples]
            batch_tensor = torch.stack(batch_sequences_list).to(device)
            
            # æ¨¡å‹é¢„æµ‹ï¼Œè¾“å‡º [B, Seq_Len]
            probs = model(batch_tensor).cpu().numpy()
            
            # æ ¸å¿ƒæ­¥éª¤ï¼šå°†é‡å çš„é¢„æµ‹ç»“æœç´¯åŠ åˆ°å¯¹åº”çš„ç‰‡æ®µä¸Š
            for j, seq_probs in enumerate(probs):
                # å½“å‰åºåˆ—åœ¨ `all_sequences` ä¸­çš„èµ·å§‹ç´¢å¼•
                original_sequence_index = i + j
                for k, prob in enumerate(seq_probs):
                    # è¿™ä¸ªæ¦‚ç‡å¯¹åº”äº `all_segments` ä¸­çš„å“ªä¸ªç‰‡æ®µ
                    original_segment_index = original_sequence_index + k
                    segment_predictions[original_segment_index] += prob
                    segment_counts[original_segment_index] += 1
    
    # å¯¹é‡å é¢„æµ‹è¿›è¡Œå¹³å‡
    # é¿å…é™¤ä»¥0
    segment_counts[segment_counts == 0] = 1 
    final_segment_probs = segment_predictions / segment_counts

    # 6. åå¤„ç†å¹¶è¾“å‡ºç»“æœ
    print("ğŸ“ˆ åå¤„ç†ç»“æœ...")
    
    # å°†æ¦‚ç‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶é¢„æµ‹
    final_segment_labels = (final_segment_probs > args.threshold).astype(int)

    # åˆå¹¶è¿ç»­çš„å¹¿å‘Šç‰‡æ®µ
    ad_timestamps = []
    is_in_ad = False
    ad_start_time = 0

    for i, label in enumerate(final_segment_labels):
        segment_start_sec = (i * step_samples) / args.sample_rate
        segment_end_sec = segment_start_sec + args.segment_duration

        if label == 1 and not is_in_ad:
            is_in_ad = True
            ad_start_time = segment_start_sec
        elif label == 0 and is_in_ad:
            is_in_ad = False
            ad_timestamps.append({"startTime": round(ad_start_time, 2), "endTime": round(segment_start_sec + (args.segment_duration / 2), 2)}) # å–ä¸Šä¸€ä¸ªç‰‡æ®µçš„ä¸­é—´ä½œä¸ºç»“æŸ

    # å¦‚æœéŸ³é¢‘ä»¥å¹¿å‘Šç»“æŸ
    if is_in_ad:
        last_segment_start = ((len(final_segment_labels) - 1) * step_samples) / args.sample_rate
        ad_timestamps.append({"startTime": round(ad_start_time, 2), "endTime": round(last_segment_start + args.segment_duration, 2)})

    # 7. ä¿å­˜ç»“æœ
    output_data = {
        "audioPath": os.path.basename(args.audio_path),
        "detectedAds": ad_timestamps
    }

    with open(args.output_json, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
        
    print(f"âœ… æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {args.output_json}")
    print("æ£€æµ‹åˆ°çš„å¹¿å‘Šæ—¶é—´æˆ³:")
    for ad in ad_timestamps:
        print(f"  - å¼€å§‹: {ad['startTime']:.2f}s, ç»“æŸ: {ad['endTime']:.2f}s")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šæ£€æµ‹æ¨¡å‹æ¨ç†è„šæœ¬")
    # --- è·¯å¾„å‚æ•° ---
    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ (.pth)')
    parser.add_argument('--audio_path', type=str, required=True, help='éœ€è¦æ£€æµ‹çš„å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_json', type=str, default='detected_ads.json', help='è¾“å‡ºç»“æœçš„JSONæ–‡ä»¶è·¯å¾„')
    
    # --- æ¨¡å‹å’Œæ•°æ®å‚æ•° (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´) ---
    parser.add_argument('--segment_duration', type=float, default=3.0, help='æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--sequence_length', type=int, default=8, help='Transformerçš„ä¸Šä¸‹æ–‡çª—å£å¤§å° (ç‰‡æ®µæ•°é‡)')
    parser.add_argument('--sample_rate', type=int, default=16000, help='éŸ³é¢‘é‡‡æ ·ç‡')
    
    # --- æ¨ç†æ§åˆ¶å‚æ•° ---
    parser.add_argument('--batch_size', type=int, default=16, help='æ¨ç†æ—¶ä½¿ç”¨çš„æ‰¹å¤„ç†å¤§å°ï¼Œä»¥é˜²æ˜¾å­˜ä¸è¶³')
    parser.add_argument('--threshold', type=float, default=0.5, help='åˆ¤æ–­ä¸ºå¹¿å‘Šçš„æ¦‚ç‡é˜ˆå€¼')
    
    args = parser.parse_args()
    
    run_inference(args)
import os
import json
import argparse
import subprocess
import tempfile

import torch
import torchaudio
import dasheng
import numpy as np
from torch import nn

# ===================================================================
# æ¨¡å‹ç±»å®šä¹‰ (ä¸è®­ç»ƒä»£ç å¯¹é½)
# ===================================================================
class ContextualAdClassifier(nn.Module):
    def __init__(self, backbone, freeze_backbone=True, d_model=768, nhead=8, num_layers=3):
        super().__init__()
        self.backbone = backbone
        self.freeze_backbone = freeze_backbone

        if self.freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        self.d_model = self.backbone.embed_dim

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, 
            nhead=nhead, 
            dim_feedforward=self.d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, 1)
        )

    def forward(self, x: torch.Tensor):
        B, S, C, L = x.shape
        x = x.view(B * S, C, L).squeeze(1)
        
        with torch.set_grad_enabled(not self.freeze_backbone):
            sequence_features = self.backbone(x) 
            segment_embeddings = sequence_features.mean(dim=1)

        segment_embeddings = segment_embeddings.view(B, S, -1)
        contextual_embeddings = self.transformer_encoder(segment_embeddings)
        logits = self.classifier(contextual_embeddings).squeeze(-1)
        
        # <--- ä¿®æ”¹: è¿”å›å€¼æ˜¯é¢„æµ‹çš„æ¯”ä¾‹ (Prediction/Ratio), è€Œéåˆ†ç±»æ¦‚ç‡ (Probability)
        predictions = torch.sigmoid(logits)
        return predictions

# ===================================================================
# å¹¿å‘Šæ£€æµ‹å™¨ç±» (å·²æ›´æ–°ä»¥é€‚åº”å›å½’ä»»åŠ¡)
# ===================================================================
class AdDetector:
    def __init__(self, model_path, device=None, segment_duration=3.0, sequence_length=8, 
                 sample_rate=16000, batch_size=16, threshold=0.5, min_ad_duration=0, 
                 merge_gap_duration=0, overlap_ratio=0.5):
        """åˆå§‹åŒ–å¹¿å‘Šæ£€æµ‹å™¨
        
        Args:
            model_path (str): æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            ...
            threshold (float): ç”¨äºåˆ¤æ–­ä¸€ä¸ªç‰‡æ®µæ˜¯å¦ä¸ºå¹¿å‘Šçš„é¢„æµ‹æ¯”ä¾‹é˜ˆå€¼
            merge_gap_duration (float): åˆå¹¶å¹¿å‘Šçš„æœ€å¤§é—´éš”ï¼ˆç§’ï¼‰
            overlap_ratio (float): éŸ³é¢‘åˆ‡ç‰‡çš„é‡å ç‡, èŒƒå›´ [0.0, 1.0)
        """
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.segment_duration = segment_duration
        self.sequence_length = sequence_length
        self.sample_rate = sample_rate
        self.batch_size = batch_size
        self.threshold = threshold
        self.min_ad_duration = min_ad_duration
        self.merge_gap_duration = merge_gap_duration
        self.overlap_ratio = overlap_ratio
        
        # åŠ è½½æ¨¡å‹
        self.backbone = dasheng.dasheng_base()
        self.model = ContextualAdClassifier(
            backbone=self.backbone, 
            freeze_backbone=False, 
            num_layers=3, 
            nhead=8
        ).to(self.device)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.eval()

    def _load_audio(self, audio_path):
        """åŠ è½½å¹¶é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
        temp_wav_path = None
        # å¦‚æœæ˜¯mp3ï¼Œå…ˆç”¨ffmpegè½¬ç ä¸ºwav
        if audio_path.lower().endswith('.mp3'):
            tmp_f = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            temp_wav_path = tmp_f.name
            tmp_f.close()
            
            ffmpeg_cmd = ['ffmpeg', '-y', '-i', audio_path, '-ar', str(self.sample_rate), '-ac', '1', temp_wav_path]
            try:
                subprocess.run(ffmpeg_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            except (subprocess.CalledProcessError, FileNotFoundError) as e:
                if os.path.exists(temp_wav_path): 
                    os.remove(temp_wav_path)
                raise RuntimeError("ffmpegè½¬ç å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ffmpegå¹¶æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„") from e
            
            audio_path = temp_wav_path
            
        try:
            waveform, sr = torchaudio.load(audio_path)
        finally:
            if temp_wav_path and os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)
                
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)
            waveform = resampler(waveform)
            
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
            
        return waveform, self.sample_rate

    def _process_segments(self, waveform):
        """å¤„ç†éŸ³é¢‘ç‰‡æ®µ"""
        seg_len_samples = int(self.segment_duration * self.sample_rate)
        
        # æ ¹æ®é‡å ç‡è®¡ç®—æ­¥é•¿
        step_samples = int(seg_len_samples * (1 - self.overlap_ratio))
        if step_samples < 1:
            step_samples = 1

        total_samples = waveform.shape[1]
        
        all_segments = [
            waveform[:, s:s + seg_len_samples] 
            for s in range(0, total_samples - seg_len_samples + 1, step_samples)
        ]
        
        if not all_segments:
            raise ValueError("éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•åˆ‡åˆ†å‡ºä»»ä½•ç‰‡æ®µ")
            
        all_sequences = []
        if len(all_segments) >= self.sequence_length:
            for i in range(len(all_segments) - self.sequence_length + 1):
                all_sequences.append(torch.stack(all_segments[i:i + self.sequence_length]))
        else:
            # å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨paddingæ¥å¤„ç†çŸ­éŸ³é¢‘ï¼Œä½†è¿™é‡Œä¸ºä¿æŒä¸è®­ç»ƒä¸€è‡´ï¼Œå…ˆæŠ›å‡ºé”™è¯¯
            raise ValueError(f"éŸ³é¢‘ç‰‡æ®µæ•° ({len(all_segments)}) ä¸è¶³ {self.sequence_length}ï¼Œæ— æ³•è¿›è¡Œæ¨ç†")
            
        return all_segments, all_sequences, step_samples

    def _run_inference(self, all_sequences, all_segments):
        """è¿è¡Œæ¨¡å‹æ¨ç†"""
        # <--- ä¿®æ”¹: å˜é‡ååæ˜ å…¶å†…å®¹æ˜¯é¢„æµ‹å€¼çš„ç´¯åŠ 
        segment_prediction_sums = np.zeros(len(all_segments))
        segment_counts = np.zeros(len(all_segments))
        
        with torch.no_grad():
            for i in range(0, len(all_sequences), self.batch_size):
                batch_tensor = torch.stack(all_sequences[i:i + self.batch_size]).to(self.device)
                # <--- ä¿®æ”¹: modelè¿”å›çš„æ˜¯ predictions
                predictions = self.model(batch_tensor).cpu().numpy()
                
                for j, seq_preds in enumerate(predictions):
                    original_sequence_index = i + j
                    for k, pred in enumerate(seq_preds):
                        original_segment_index = original_sequence_index + k
                        segment_prediction_sums[original_segment_index] += pred
                        segment_counts[original_segment_index] += 1
                        
        segment_counts[segment_counts == 0] = 1
        # <--- ä¿®æ”¹: è¿”å›å¹³å‡åçš„æœ€ç»ˆé¢„æµ‹æ¯”ä¾‹
        final_segment_predictions = segment_prediction_sums / segment_counts
        return final_segment_predictions

    def _post_process(self, final_segment_predictions, step_samples):
        """åå¤„ç†æ¨ç†ç»“æœ"""
        # åŸºäºé˜ˆå€¼å°†è¿ç»­çš„é¢„æµ‹æ¯”ä¾‹è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡ç­¾
        final_segment_labels = (final_segment_predictions > self.threshold).astype(int)
        ad_timestamps = []
        is_in_ad = False
        ad_start_time = 0
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        for i, label in enumerate(final_segment_labels):
            segment_start_sec = (i * step_samples) / self.sample_rate
            if label == 1 and not is_in_ad:
                is_in_ad = True
                ad_start_time = segment_start_sec
            elif label == 0 and is_in_ad:
                is_in_ad = False
                # ç»“æŸæ—¶é—´åº”è¯¥åŸºäºç‰‡æ®µçš„ä¸­å¿ƒæˆ–å¼€å§‹ï¼Œè¿™é‡Œç”¨å¼€å§‹+ç‰‡æ®µæ—¶é•¿ä½œä¸ºè¿‘ä¼¼
                ad_end_time = segment_start_sec + (self.segment_duration - (step_samples / self.sample_rate))
                ad_timestamps.append({
                    "startTime": round(ad_start_time, 2),
                    "endTime": round(ad_end_time, 2)
                })
                
        if is_in_ad:
            last_segment_start = ((len(final_segment_labels) - 1) * step_samples) / self.sample_rate
            ad_end_time = last_segment_start + self.segment_duration
            ad_timestamps.append({
                "startTime": round(ad_start_time, 2),
                "endTime": round(ad_end_time, 2)
            })
            
        # è¿‡æ»¤çŸ­å¹¿å‘Š
        if self.min_ad_duration > 0 and ad_timestamps:
            ad_timestamps = [
                ad for ad in ad_timestamps 
                if (ad['endTime'] - ad['startTime']) >= self.min_ad_duration
            ]
            
        # åˆå¹¶ç›¸è¿‘å¹¿å‘Š
        if self.merge_gap_duration > 0 and len(ad_timestamps) > 1:
            merged_ads = [ad_timestamps[0]]
            for current_ad in ad_timestamps[1:]:
                last_merged_ad = merged_ads[-1]
                gap = current_ad['startTime'] - last_merged_ad['endTime']
                if gap <= self.merge_gap_duration:
                    last_merged_ad['endTime'] = current_ad['endTime']
                else:
                    merged_ads.append(current_ad)
            ad_timestamps = merged_ads
            
        # ç”Ÿæˆæ¯ä¸ªç‰‡æ®µçš„è¯¦ç»†ç»“æœ
        segment_results = []
        # <--- ä¿®æ”¹: è¿­ä»£çš„æ˜¯ final_segment_predictions
        for i, pred in enumerate(final_segment_predictions):
            segment_start_sec = (i * step_samples) / self.sample_rate
            segment_end_sec = segment_start_sec + self.segment_duration
            segment_results.append({
                "startTime": round(segment_start_sec, 2),
                "endTime": round(segment_end_sec, 2),
                # <--- ä¿®æ”¹: é”®åä¸º "ad_ratio"ï¼Œå€¼æ¥è‡ª pred
                "ad_ratio": round(float(pred), 4),
                "isAd": bool(pred > self.threshold)
            })
            
        return ad_timestamps, segment_results

    def detect(self, audio_path, output_json=None):
        """æ£€æµ‹éŸ³é¢‘æ–‡ä»¶ä¸­çš„å¹¿å‘Š"""
        # 1. åŠ è½½éŸ³é¢‘
        waveform, _ = self._load_audio(audio_path)
        
        # 2. å¤„ç†ç‰‡æ®µ
        all_segments, all_sequences, step_samples = self._process_segments(waveform)
        
        # 3. è¿è¡Œæ¨ç†
        # <--- ä¿®æ”¹: å˜é‡åå¯¹é½
        final_segment_predictions = self._run_inference(all_sequences, all_segments)
        
        # 4. åå¤„ç†
        # <--- ä¿®æ”¹: ä¼ å…¥å˜é‡åå¯¹é½
        ad_timestamps, segment_results = self._post_process(final_segment_predictions, step_samples)
        
        # 5. æ•´ç†ç»“æœ
        output_data = {
            "audioPath": os.path.basename(audio_path),
            "detectedAds": ad_timestamps,
            "segmentResults": segment_results
        }
        
        # 6. ä¿å­˜ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
        if output_json:
            with open(output_json, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=4, ensure_ascii=False)
                
        return output_data

def format_time(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸º 'MM:SS.xx (seconds)' æ ¼å¼çš„å­—ç¬¦ä¸²"""
    minutes = int(seconds) // 60
    secs = seconds % 60
    return f"{minutes:02d}:{secs:05.2f} ({seconds:.2f}s)"

def run_inference(args):
    """å‘½ä»¤è¡Œæ¥å£çš„æ¨ç†å‡½æ•°"""
    print(f"ğŸ”§ åˆå§‹åŒ–å¹¿å‘Šæ£€æµ‹å™¨...")
    print(f" Â  - æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f" Â  - éŸ³é¢‘åˆ‡ç‰‡é‡å ç‡: {args.overlap_ratio * 100:.0f}%")
    
    detector = AdDetector(
        model_path=args.model_path,
        segment_duration=args.segment_duration,
        sequence_length=args.sequence_length,
        sample_rate=args.sample_rate,
        batch_size=args.batch_size,
        threshold=args.threshold,
        min_ad_duration=args.min_ad_duration,
        merge_gap_duration=args.merge_gap_duration,
        overlap_ratio=args.overlap_ratio
    )
    
    print(f"\nğŸ§ å¼€å§‹æ£€æµ‹éŸ³é¢‘æ–‡ä»¶: {args.audio_path}")
    try:
        result = detector.detect(args.audio_path, args.output_json)
        
        print("\n" + "="*50)
        if result['detectedAds']:
            print("âœ… æ£€æµ‹å®Œæˆ! æœ€ç»ˆå¹¿å‘Šæ—¶é—´æˆ³:")
            for ad in result['detectedAds']:
                duration = ad['endTime'] - ad['startTime']
                print(f" Â  - å¼€å§‹: {format_time(ad['startTime'])}, "
                      f"ç»“æŸ: {format_time(ad['endTime'])} "
                      f"(æ—¶é•¿: {format_time(duration)})")
        else:
            print("âœ… æ£€æµ‹å®Œæˆ! æœªæ£€æµ‹åˆ°æ»¡è¶³æ¡ä»¶çš„å¹¿å‘Šã€‚")
        print("="*50)

        if args.output_json:
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {args.output_json}")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        # <--- ä¿®æ”¹: æè¿°æ›´æ–°ä¸ºå›å½’ä»»åŠ¡
        description="ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šæ¯”ä¾‹é¢„æµ‹æ¨¡å‹æ¨ç†è„šæœ¬",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ (.pth)')
    parser.add_argument('--audio_path', type=str, required=True, help='éœ€è¦æ£€æµ‹çš„å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„ (æ”¯æŒ .wav, .mp3)')
    parser.add_argument('--output_json', type=str, default=None, help='è¾“å‡ºç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„ (å¯é€‰, ä¸æŒ‡å®šåˆ™ä¸ä¿å­˜)')
    parser.add_argument('--segment_duration', type=float, default=3.0, help='(åº”ä¸è®­ç»ƒæ—¶ä¸€è‡´) æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--sequence_length', type=int, default=8, help='(åº”ä¸è®­ç»ƒæ—¶ä¸€è‡´) Transformer çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°')
    
    parser.add_argument('--overlap_ratio', type=float, default=0.5, 
                        help='(åº”ä¸è®­ç»ƒæ—¶ä¸€è‡´) éŸ³é¢‘åˆ‡ç‰‡çš„é‡å ç‡ (ä¾‹å¦‚ 0.5 è¡¨ç¤º 50%% é‡å )')

    parser.add_argument('--sample_rate', type=int, default=16000, help='éŸ³é¢‘é‡‡æ ·ç‡')
    parser.add_argument('--batch_size', type=int, default=16, help='æ¨ç†æ—¶ä½¿ç”¨çš„æ‰¹å¤„ç†å¤§å°')
    # <--- ä¿®æ”¹: æ›´æ–°thresholdçš„å¸®åŠ©æ–‡æœ¬
    parser.add_argument('--threshold', type=float, default=0.5, help='åˆ¤æ–­ä¸ºå¹¿å‘Šçš„é¢„æµ‹æ¯”ä¾‹é˜ˆå€¼')
    parser.add_argument('--min_ad_duration', type=float, default=0, 
                        help='(åå¤„ç†) è¿‡æ»¤æ‰æ—¶é•¿å°äºæ­¤å€¼(ç§’)çš„å¹¿å‘Šå—ã€‚\n'
                             'é»˜è®¤ä¸º0ï¼Œä¸è¿‡æ»¤ã€‚')
    parser.add_argument('--merge_gap_duration', type=float, default=0, 
                        help='(åå¤„ç†) åˆå¹¶é—´éš”å°äºæ­¤å€¼(ç§’)çš„å¹¿å‘Šå—ã€‚\n'
                             'é»˜è®¤ä¸º0ï¼Œä¸åˆå¹¶ã€‚')
    
    args = parser.parse_args()
    
    run_inference(args)
import os
import torch
import torchaudio
import subprocess
import argparse
from train import DashengAdClassifier  # å‡è®¾è®­ç»ƒè„šæœ¬æ–‡ä»¶åä¸º train.pyï¼Œå…¶ä¸­å®šä¹‰äº† DashengAdClassifier

# è®¾ç½® torchaudio ä½¿ç”¨ sox_io åç«¯ï¼Œä»¥æ”¯æŒ mp3 æ ¼å¼
torchaudio.set_audio_backend("sox_io")


def convert_mp3_to_wav(mp3_path):
    """
    å¦‚æœè¾“å…¥æ˜¯ .mp3 æ–‡ä»¶ï¼Œåˆ™è°ƒç”¨ ffmpeg è½¬ä¸º .wav å¹¶è¿”å›æ–°çš„è·¯å¾„ã€‚
    """
    wav_path = mp3_path.replace(".mp3", ".wav")
    if not os.path.exists(wav_path):
        print(f"âš™ï¸ å‘ç° mp3 æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸º wav: {wav_path}")
        subprocess.run(["ffmpeg", "-y", "-i", mp3_path, wav_path], check=True)
    return wav_path


def segment_audio(waveform, sample_rate, segment_duration=5.0):
    """
    å°† waveform æŒ‰ç…§å›ºå®šæ—¶é•¿ segment_durationï¼ˆç§’ï¼‰åˆ‡æˆè‹¥å¹²ä¸é‡å çš„ç‰‡æ®µï¼Œ
    è¿”å›åˆ—è¡¨ segmentsï¼ˆæ¯é¡¹ Tensor[1, segment_samples]ï¼‰å’Œå¯¹åº”çš„æ—¶é—´æˆ³åˆ—è¡¨ timestampsï¼ˆ(start_sec, end_sec)ï¼‰ã€‚
    """
    segment_samples = int(segment_duration * sample_rate)
    total_samples = waveform.shape[1]
    segments = []
    timestamps = []

    for start in range(0, total_samples - segment_samples + 1, segment_samples):
        end = start + segment_samples
        segment = waveform[:, start:end]
        if segment.shape[1] == segment_samples:
            segments.append(segment)
            timestamps.append((start / sample_rate, end / sample_rate))
    return segments, timestamps


def format_timestamp(seconds):
    """
    å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS æ ¼å¼
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{seconds:05.2f}"


def predict_ad_segments(audio_path, model_path, threshold=0.5, segment_duration=5.0):
    """
    å¯¹æ•´æ®µéŸ³é¢‘åšå¹¿å‘Šç‰‡æ®µæ£€æµ‹ï¼Œè¿”å›å¹¿å‘Šæ—¶é—´æˆ³åˆ—è¡¨ã€‚
    åªæ‰“å° Sigmoid åçš„æ¦‚ç‡å€¼ï¼Œä¸å†æ‰“å° Logitã€‚
    """
    sample_rate = 16000

    # å¦‚æœæ˜¯ mp3ï¼Œå…ˆè½¬æˆ wav
    if audio_path.lower().endswith(".mp3"):
        audio_path = convert_mp3_to_wav(audio_path)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"âœ… éŸ³é¢‘è·¯å¾„: {audio_path}")

    # 1. è¯»å–éŸ³é¢‘å¹¶é‡é‡‡æ ·ã€è½¬æ¢å•å£°é“
    waveform, sr = torchaudio.load(audio_path)
    if sr != sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)
        waveform = resampler(waveform)
    waveform = waveform.mean(dim=0, keepdim=True)  # è½¬ä¸ºå•å£°é“
    total_duration = waveform.shape[1] / sample_rate  # æ€»æ—¶é•¿ï¼ˆç§’ï¼‰

    # 2. åˆ‡åˆ†æˆè‹¥å¹²ä¸ªå›ºå®šæ—¶é•¿çš„ segment
    segments, timestamps = segment_audio(waveform, sample_rate, segment_duration)

    # 3. åŠ è½½æ¨¡å‹ï¼Œå¹¶è®¾ç½®ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ segment_duration
    model = DashengAdClassifier(freeze_dasheng=False,
                                segment_duration=segment_duration).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    ad_timestamps = []
    with torch.no_grad():
        for segment, (start_sec, end_sec) in zip(segments, timestamps):
            # segment: Tensor[1, segment_samples]ï¼Œå…ˆåœ¨å‰é¢åŠ  batch ç»´åº¦
            input_tensor = segment.unsqueeze(0).to(device)  # [1, 1, seg_len_samples]

            # æ„é€  seg_start å’Œ total_duration ä¸¤ä¸ª Tensor
            seg_start_tensor = torch.tensor([start_sec], dtype=torch.float32, device=device)       # [1]
            total_d_tensor = torch.tensor([total_duration], dtype=torch.float32, device=device)    # [1]

            # å‰å‘è®¡ç®—å¾—åˆ° logitï¼Œå†ç”¨ sigmoid è½¬æˆæ¦‚ç‡
            logits = model(input_tensor, seg_start_tensor, total_d_tensor)  # [1]
            prob = torch.sigmoid(logits).item()

            # æ‰“å°æ¦‚ç‡
            if prob >= threshold:
                ad_timestamps.append((start_sec, end_sec))
                print(f"âœ… å¹¿å‘Šç‰‡æ®µ {format_timestamp(start_sec)} ({start_sec:.2f}s) ~ {format_timestamp(end_sec)} ({end_sec:.2f}s)ï¼Œæ¦‚ç‡={prob:.4f}")
            else:
                print(f"{format_timestamp(start_sec)} ({start_sec:.2f}s) ~ {format_timestamp(end_sec)} ({end_sec:.2f}s)ï¼Œæ¦‚ç‡={prob:.4f}")

    return ad_timestamps


def merge_continuous_segments(segments, gap=0.0):
    """
    åˆå¹¶è¿ç»­æˆ–æ¥è¿‘çš„å¹¿å‘Šç‰‡æ®µã€‚ä¾‹å¦‚ [(0,5), (5,10)] ä»¥åŠ gap=0 ä¼šåˆå¹¶æˆ [(0,10)]ã€‚
    """
    if not segments:
        return []

    merged = [segments[0]]
    for curr_start, curr_end in segments[1:]:
        prev_start, prev_end = merged[-1]
        if curr_start - prev_end <= gap:
            merged[-1] = (prev_start, max(prev_end, curr_end))
        else:
            merged.append((curr_start, curr_end))
    return merged


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¹¿å‘Šæ£€æµ‹æ¨ç†è„šæœ¬ï¼ˆåªæ‰“å°æ¦‚ç‡ï¼‰")
    parser.add_argument('--audio', type=str, required=True, help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ mp3 æˆ– wavï¼‰')
    parser.add_argument('--model', type=str, default='best_model.pth', help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--threshold', type=float, default=0.5, help='åˆ¤æ–­é˜ˆå€¼ï¼ˆSigmoid åæ¦‚ç‡ â‰¥ é˜ˆå€¼å³è§†ä¸ºå¹¿å‘Šï¼‰')
    parser.add_argument('--merge_gap', type=float, default=0.0,
                        help='åˆå¹¶è¿ç»­å¹¿å‘Šç‰‡æ®µçš„æœ€å¤§é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º 0ï¼Œå³ä¸¥æ ¼ç›¸è¿æ‰åˆå¹¶')
    parser.add_argument('--segment_duration', type=float, default=4.0,
                        help='åˆ‡ç‰‡æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œè¦ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼Œé»˜è®¤ 4.0s')
    args = parser.parse_args()

    # é¢„æµ‹å¹¶åˆå¹¶
    raw_results = predict_ad_segments(
        audio_path=args.audio,
        model_path=args.model,
        threshold=args.threshold,
        segment_duration=args.segment_duration
    )
    merged_results = merge_continuous_segments(raw_results, gap=args.merge_gap)

    print("\nğŸš¨ è¯†åˆ«å‡ºçš„å¹¿å‘Šæ—¶é—´æ®µï¼ˆåˆå¹¶åï¼‰ï¼š")
    for start, end in merged_results:
        print(f"å¹¿å‘Šï¼š{format_timestamp(start)} ({start:.2f}s) ~ {format_timestamp(end)} ({end:.2f}s)")

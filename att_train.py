import os
import json
import argparse
import torch
import torchaudio
import dasheng
import random
import numpy as np
from torch import nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch.nn.functional as F

# ----------------------------
# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
# ----------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed()

# ----------------------------
# è‡ªå®šä¹‰æ•°æ®é›† (ä¿®æ”¹å): æä¾›è¿ç»­çš„ç‰‡æ®µåºåˆ—ä»¥æ•è·ä¸Šä¸‹æ–‡
# ----------------------------
class ContextualAdDataset(Dataset):
    def __init__(self,
                 json_path: str,
                 audio_dir: str,
                 sequence_length: int = 8,
                 segment_duration: float = 3.0,
                 sample_rate: int = 16000,
                 ad_ratio_threshold: float = 0.5,
                 max_files: int = -1):
        """
        json_path: æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„
        audio_dir: éŸ³é¢‘æ–‡ä»¶æ‰€åœ¨ç›®å½•
        sequence_length: æ¯ä¸ªæ ·æœ¬åŒ…å«çš„è¿ç»­ç‰‡æ®µæ•°é‡ (ç”¨äºTransformerä¸Šä¸‹æ–‡)
        segment_duration: æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰
        sample_rate: ç›®æ ‡é‡‡æ ·ç‡
        ad_ratio_threshold: å¹¿å‘Šå æ¯”è¶…è¿‡æ­¤é˜ˆå€¼åˆ™æ ‡è®°ä¸ºå¹¿å‘Š (1), å¦åˆ™ä¸ºéå¹¿å‘Š (0)
        max_files: è‹¥ >0, åˆ™åªåŠ è½½å‰ max_files ä¸ªéŸ³é¢‘æ–‡ä»¶, ç”¨äºå¿«é€Ÿæµ‹è¯•
        """
        self.sequence_length = sequence_length
        self.segment_duration = segment_duration
        self.sample_rate = sample_rate
        self.seg_len_samples = int(segment_duration * sample_rate)
        self.ad_ratio_threshold = ad_ratio_threshold
        self.sequences = []

        # 1. æŒ‰éŸ³é¢‘æ–‡ä»¶åˆ†ç»„å¤„ç†
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        files_to_process = data
        if max_files > 0:
            files_to_process = data[:max_files]

        for item in tqdm(files_to_process, desc="åŠ è½½å¹¶åˆ‡åˆ†æ•°æ®", unit="æ–‡ä»¶"):
            audio_path = os.path.join(audio_dir, item['audioPath'])
            if not os.path.exists(audio_path):
                continue

            try:
                waveform, sr = torchaudio.load(audio_path)
            except Exception as e:
                print(f"Error loading {audio_path}: {e}")
                continue

            # é‡é‡‡æ ·å’Œè½¬å•å£°é“
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)
                waveform = resampler(waveform)
            waveform = waveform.mean(dim=0, keepdim=True)
            total_duration_samples = waveform.shape[1]
            
            ad_intervals = sorted([(ad['startTime'], ad['endTime']) for ad in item.get('ads', [])], key=lambda x: x[0])

            # 2. ç”Ÿæˆè¯¥æ–‡ä»¶çš„æ‰€æœ‰è¿ç»­ç‰‡æ®µåŠå…¶æ ‡ç­¾
            file_segments = []
            step_samples = self.seg_len_samples // 2  # 50% é‡å 
            for start_sample in range(0, total_duration_samples - self.seg_len_samples + 1, step_samples):
                end_sample = start_sample + self.seg_len_samples
                segment_waveform = waveform[:, start_sample:end_sample]

                # è®¡ç®—å¹¿å‘Šé‡å ç‡
                seg_start_sec = start_sample / self.sample_rate
                seg_end_sec = end_sample / self.sample_rate
                ad_overlap_sec = 0.0
                for ad_start, ad_end in ad_intervals:
                    overlap_start = max(seg_start_sec, ad_start)
                    overlap_end = min(seg_end_sec, ad_end)
                    if overlap_end > overlap_start:
                        ad_overlap_sec += (overlap_end - overlap_start)
                
                ad_ratio = ad_overlap_sec / self.segment_duration
                # æ ¹æ®é˜ˆå€¼ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾
                label = 1 if ad_ratio >= self.ad_ratio_threshold else 0
                
                file_segments.append({'waveform': segment_waveform, 'label': torch.tensor(label, dtype=torch.float32)})
            
            # 3. ä»è¯¥æ–‡ä»¶çš„ç‰‡æ®µåˆ—è¡¨ä¸­æ„å»ºåºåˆ—
            if len(file_segments) >= self.sequence_length:
                for i in range(len(file_segments) - self.sequence_length + 1):
                    self.sequences.append(file_segments[i:i + self.sequence_length])

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx: int):
        sequence_data = self.sequences[idx]
        
        waveforms = torch.stack([s['waveform'] for s in sequence_data]) # [Seq_Len, 1, Samples]
        labels = torch.stack([s['label'] for s in sequence_data])       # [Seq_Len]
        
        return waveforms, labels

# ----------------------------
# ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šåˆ†ç±»æ¨¡å‹ (ä¿®æ­£å)
# ----------------------------
class ContextualAdClassifier(nn.Module):
    def __init__(self, backbone, freeze_backbone=True, d_model=768, nhead=8, num_layers=3):
        super().__init__()
        self.backbone = backbone
        self.freeze_backbone = freeze_backbone

        if self.freeze_backbone:
            print("ğŸš« å†»ç»“ dasheng éª¨å¹²æ‰€æœ‰å‚æ•°")
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        # Dasheng è¾“å‡ºçš„ç‰¹å¾ç»´åº¦
        self.d_model = self.backbone.embed_dim

        # Transformer Encoder Layer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, 
            nhead=nhead, 
            dim_feedforward=self.d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True  # é‡è¦: è¾“å…¥æ ¼å¼ä¸º [B, Seq_Len, Dim]
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, 1)
        )

    def forward(self, x: torch.Tensor):
        """
        x: [B, Seq_Len, 1, Samples] -> ä¸€æ‰¹éŸ³é¢‘ç‰‡æ®µåºåˆ—
        è¿”å›: [B, Seq_Len] -> æ¯ä¸ªç‰‡æ®µæ˜¯å¹¿å‘Šçš„æ¦‚ç‡
        """
        B, S, C, L = x.shape
        
        # 1. å°†æ‰¹æ¬¡å’Œåºåˆ—ç»´åº¦åˆå¹¶, ä»¥ä¾¿æ‰¹é‡é€šè¿‡backbone
        x = x.view(B * S, C, L).squeeze(1) # -> [B*S, Samples]
        
        # 2. ä½¿ç”¨ dasheng æå–æ¯ä¸ªç‰‡æ®µçš„æ—¶åºç‰¹å¾
        with torch.set_grad_enabled(not self.freeze_backbone):
            # ----------- ã€é”™è¯¯çš„ä»£ç  - å·²æ³¨é‡Šã€‘ -----------
            # segment_embeddings = self.backbone.forward_cls_token(x) # -> [B*S, D]
            
            # ----------- ã€ä¿®æ­£åçš„ä»£ç ã€‘ -----------
            # self.backbone(x) è¿”å›æ—¶åºç‰¹å¾ [B*S, TimeSteps, Dim]
            sequence_features = self.backbone(x) 
            # é€šè¿‡åœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œå¹³å‡æ± åŒ–, å¾—åˆ°æ¯ä¸ªç‰‡æ®µçš„å•ä¸€ç‰¹å¾å‘é‡
            segment_embeddings = sequence_features.mean(dim=1) # -> [B*S, Dim]
            # ----------------------------------------

        # 3. æ¢å¤åºåˆ—ç»´åº¦
        segment_embeddings = segment_embeddings.view(B, S, -1) # -> [B, S, D]
        
        # 4. é€šè¿‡ Transformer Encoder èåˆä¸Šä¸‹æ–‡ä¿¡æ¯
        contextual_embeddings = self.transformer_encoder(segment_embeddings) # -> [B, S, D]
        
        # 5. é€šè¿‡åˆ†ç±»å¤´å¾—åˆ°æ¯ä¸ªç‰‡æ®µçš„ logits
        logits = self.classifier(contextual_embeddings).squeeze(-1) # -> [B, S]
        
        # 6. ä½¿ç”¨ Sigmoid è·å¾—æ¦‚ç‡
        probs = torch.sigmoid(logits)
        
        return probs
# ----------------------------
# è®­ç»ƒä¸éªŒè¯å‡½æ•° (æ›´æ–°å)
# ----------------------------
def train_one_epoch(model: nn.Module,
                    dataloader: DataLoader,
                    criterion: nn.Module,
                    optimizer: torch.optim.Optimizer,
                    device: torch.device) -> float:
    model.train()
    total_loss = 0.0
    
    bar = tqdm(dataloader, desc="ğŸš€ Training", leave=False)
    for waveforms, labels in bar:
        # waveforms: [B, S, 1, L], labels: [B, S]
        waveforms = waveforms.to(device)
        labels = labels.to(device)
        
        optimizer.zero_grad()
        
        # è·å¾—æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡ [B, S]
        predictions = model(waveforms)
        
        # å°†é¢„æµ‹å’Œæ ‡ç­¾å±•å¹³, ä»¥ä¾¿è®¡ç®—æŸå¤±
        loss = criterion(predictions.view(-1), labels.view(-1))
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # æ¢¯åº¦è£å‰ª
        optimizer.step()
        
        total_loss += loss.item()
        bar.set_postfix(loss=loss.item())
        
    return total_loss / len(dataloader)


def evaluate(model: nn.Module,
             dataloader: DataLoader,
             criterion: nn.Module,
             device: torch.device):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    bar = tqdm(dataloader, desc="ğŸ§ª Evaluating", leave=False)
    with torch.no_grad():
        for waveforms, labels in bar:
            waveforms = waveforms.to(device)
            labels = labels.to(device)
            
            predictions = model(waveforms)
            
            loss = criterion(predictions.view(-1), labels.view(-1))
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾ç”¨äºè®¡ç®—æŒ‡æ ‡
            # å°†æ¦‚ç‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶é¢„æµ‹ (0æˆ–1)
            binary_preds = (predictions > 0.5).float()
            
            all_preds.append(binary_preds.view(-1).cpu())
            all_labels.append(labels.view(-1).cpu())

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()
    
    # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    metrics = {
        "loss": total_loss / len(dataloader),
        "accuracy": accuracy_score(all_labels, all_preds),
        "precision": precision_score(all_labels, all_preds, zero_division=0),
        "recall": recall_score(all_labels, all_preds, zero_division=0),
        "f1": f1_score(all_labels, all_preds, zero_division=0)
    }
    
    return metrics

# ----------------------------
# ä¸»å‡½æ•° (æ›´æ–°å)
# ----------------------------
def main():
    parser = argparse.ArgumentParser(description="ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šæ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument('--json_path', type=str, default='./audio_ads.json', help='æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, default='./audio/', help='éŸ³é¢‘ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹é‡å¤§å° (åºåˆ—è¾ƒé•¿, å»ºè®®å‡å°)')
    parser.add_argument('--num_epochs', type=int, default=25, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--val_split', type=float, default=0.15, help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--freeze_dasheng', action='store_true', help='æ˜¯å¦å†»ç»“ dasheng éª¨å¹²')
    parser.add_argument('--max_files', type=int, default=50, help='æœ€å¤§éŸ³é¢‘æ–‡ä»¶æ•°, ç”¨äºå¿«é€Ÿæµ‹è¯• (-1 è¡¨ç¤ºæ‰€æœ‰æ–‡ä»¶)')
    parser.add_argument('--segment_duration', type=float, default=3.0, help='æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--sequence_length', type=int, default=8, help='Transformerçš„ä¸Šä¸‹æ–‡çª—å£å¤§å° (ç‰‡æ®µæ•°é‡)')
    parser.add_argument('--output_dir', type=str, default='./output_contextual', help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"======= ä½¿ç”¨è®¾å¤‡: {device} =======")
    
    os.makedirs(args.output_dir, exist_ok=True)

    # 1. åŠ è½½æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½å¹¶æ„å»ºåºåˆ—æ•°æ®é›†...")
    full_dataset = ContextualAdDataset(
        json_path=args.json_path,
        audio_dir=args.audio_dir,
        sequence_length=args.sequence_length,
        segment_duration=args.segment_duration,
        max_files=args.max_files
    )
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ - æ€»åºåˆ—æ•°: {len(full_dataset)}")

    # 2. åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    dataset_size = len(full_dataset)
    val_size = int(np.floor(args.val_split * dataset_size))
    train_size = dataset_size - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])
    
    print(f"   è®­ç»ƒé›†åºåˆ—æ•°: {len(train_dataset)} | éªŒè¯é›†åºåˆ—æ•°: {len(val_dataset)}")

    # 3. åˆ›å»º DataLoader
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨
    print("ğŸ› ï¸ åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹...")
    backbone = dasheng.dasheng_base() # ä½¿ç”¨ base ç‰ˆæœ¬ä»¥å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
    model = ContextualAdClassifier(
        backbone=backbone,
        freeze_backbone=args.freeze_dasheng
    ).to(device)
    
    criterion = nn.BCELoss() # äºŒåˆ†ç±»äº¤å‰ç†µ
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=2, verbose=True)

    # 5. è®­ç»ƒå¾ªç¯
    best_f1 = 0.0
    for epoch in range(args.num_epochs):
        print(f"\n======== Epoch {epoch+1}/{args.num_epochs} ========")
        
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_metrics = evaluate(model, val_loader, criterion, device)
        
        current_f1 = val_metrics['f1']
        
        print(f"\nEpoch {epoch+1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")
        print(f"  {'éªŒè¯å‡†ç¡®ç‡:':<15} {val_metrics['accuracy']:.4f}")
        print(f"  {'éªŒè¯ç²¾ç¡®ç‡:':<15} {val_metrics['precision']:.4f}")
        print(f"  {'éªŒè¯å¬å›ç‡:':<15} {val_metrics['recall']:.4f}")
        print(f"  {'éªŒè¯ F1 åˆ†æ•°:':<15} {val_metrics['f1']:.4f}")
        
        scheduler.step(current_f1)
        
        if current_f1 > best_f1:
            best_f1 = current_f1
            best_model_path = os.path.join(args.output_dir, 'best_model.pth')
            torch.save(model.state_dict(), best_model_path)
            print(f"\nğŸ’¾ æ–°çš„æœ€ä½³ F1 åˆ†æ•°ï¼æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³ F1 åˆ†æ•°ä¸º: {best_f1:.4f}")

if __name__ == '__main__':
    main()
import os
import json
import argparse
import torch
import torchaudio
import dasheng
import random
import numpy as np
from torch import nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch.nn.functional as F
from sklearn.model_selection import train_test_split

# ----------------------------
# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
# ----------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed()

# ----------------------------
# è‡ªå®šä¹‰æ•°æ®é›† (ä¿®æ”¹å): æä¾›è¿ç»­çš„ç‰‡æ®µåºåˆ—ä»¥æ•è·ä¸Šä¸‹æ–‡
# ----------------------------
class ContextualAdDataset(Dataset):
    def __init__(self,
                 file_items: list,  # æ”¹ä¸ºç›´æ¥ä¼ å…¥æ–‡ä»¶é¡¹åˆ—è¡¨
                 audio_dir: str,
                 sequence_length: int = 8,
                 segment_duration: float = 3.0,
                 sample_rate: int = 16000,
                 ad_ratio_threshold: float = 0.5):
        """
        file_items: æ ‡æ³¨JSONæ–‡ä»¶ä¸­çš„é¡¹åˆ—è¡¨
        audio_dir: éŸ³é¢‘æ–‡ä»¶æ‰€åœ¨ç›®å½•
        sequence_length: æ¯ä¸ªæ ·æœ¬åŒ…å«çš„è¿ç»­ç‰‡æ®µæ•°é‡ (ç”¨äºTransformerä¸Šä¸‹æ–‡)
        segment_duration: æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰
        sample_rate: ç›®æ ‡é‡‡æ ·ç‡
        ad_ratio_threshold: å¹¿å‘Šå æ¯”è¶…è¿‡æ­¤é˜ˆå€¼åˆ™æ ‡è®°ä¸ºå¹¿å‘Š (1), å¦åˆ™ä¸ºéå¹¿å‘Š (0)
        """
        self.sequence_length = sequence_length
        self.segment_duration = segment_duration
        self.sample_rate = sample_rate
        self.seg_len_samples = int(segment_duration * sample_rate)
        self.ad_ratio_threshold = ad_ratio_threshold
        self.sequences = []
        self.audio_dir = audio_dir

        # æŒ‰éŸ³é¢‘æ–‡ä»¶åˆ†ç»„å¤„ç†
        for item in tqdm(file_items, desc="åŠ è½½å¹¶åˆ‡åˆ†æ•°æ®", unit="æ–‡ä»¶"):
            audio_path = os.path.join(self.audio_dir, item['audioPath'])
            if not os.path.exists(audio_path):
                continue

            try:
                waveform, sr = torchaudio.load(audio_path)
            except Exception as e:
                print(f"Error loading {audio_path}: {e}")
                continue

            # é‡é‡‡æ ·å’Œè½¬å•å£°é“
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)
                waveform = resampler(waveform)
            waveform = waveform.mean(dim=0, keepdim=True)
            total_duration_samples = waveform.shape[1]
            
            ad_intervals = sorted([(ad['startTime'], ad['endTime']) for ad in item.get('ads', [])], key=lambda x: x[0])

            # ç”Ÿæˆè¯¥æ–‡ä»¶çš„æ‰€æœ‰è¿ç»­ç‰‡æ®µåŠå…¶æ ‡ç­¾
            file_segments = []
            step_samples = self.seg_len_samples // 2  # 50% é‡å 
            for start_sample in range(0, total_duration_samples - self.seg_len_samples + 1, step_samples):
                end_sample = start_sample + self.seg_len_samples
                segment_waveform = waveform[:, start_sample:end_sample]

                # è®¡ç®—å¹¿å‘Šé‡å ç‡
                seg_start_sec = start_sample / self.sample_rate
                seg_end_sec = end_sample / self.sample_rate
                ad_overlap_sec = 0.0
                for ad_start, ad_end in ad_intervals:
                    overlap_start = max(seg_start_sec, ad_start)
                    overlap_end = min(seg_end_sec, ad_end)
                    if overlap_end > overlap_start:
                        ad_overlap_sec += (overlap_end - overlap_start)
                
                ad_ratio = ad_overlap_sec / self.segment_duration
                # æ ¹æ®é˜ˆå€¼ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾
                label = 1 if ad_ratio >= self.ad_ratio_threshold else 0
                
                file_segments.append({'waveform': segment_waveform, 'label': torch.tensor(label, dtype=torch.float32)})
            
            # ä»è¯¥æ–‡ä»¶çš„ç‰‡æ®µåˆ—è¡¨ä¸­æ„å»ºåºåˆ—
            if len(file_segments) >= self.sequence_length:
                for i in range(len(file_segments) - self.sequence_length + 1):
                    self.sequences.append(file_segments[i:i + self.sequence_length])

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx: int):
        sequence_data = self.sequences[idx]
        
        waveforms = torch.stack([s['waveform'] for s in sequence_data]) # [Seq_Len, 1, Samples]
        labels = torch.stack([s['label'] for s in sequence_data])       # [Seq_Len]
        
        return waveforms, labels

# ----------------------------
# ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šåˆ†ç±»æ¨¡å‹
# ----------------------------
class ContextualAdClassifier(nn.Module):
    def __init__(self, backbone, freeze_backbone=True, d_model=768, nhead=8, num_layers=3):
        super().__init__()
        self.backbone = backbone
        self.freeze_backbone = freeze_backbone

        if self.freeze_backbone:
            print("ğŸš« å†»ç»“ dasheng éª¨å¹²æ‰€æœ‰å‚æ•°")
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        # Dasheng è¾“å‡ºçš„ç‰¹å¾ç»´åº¦
        self.d_model = self.backbone.embed_dim

        # Transformer Encoder Layer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, 
            nhead=nhead, 
            dim_feedforward=self.d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True  # é‡è¦: è¾“å…¥æ ¼å¼ä¸º [B, Seq_Len, Dim]
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, 1)
        )

    def forward(self, x: torch.Tensor):
        """
        x: [B, Seq_Len, 1, Samples] -> ä¸€æ‰¹éŸ³é¢‘ç‰‡æ®µåºåˆ—
        è¿”å›: [B, Seq_Len] -> æ¯ä¸ªç‰‡æ®µæ˜¯å¹¿å‘Šçš„æ¦‚ç‡
        """
        B, S, C, L = x.shape
        
        # 1. å°†æ‰¹æ¬¡å’Œåºåˆ—ç»´åº¦åˆå¹¶, ä»¥ä¾¿æ‰¹é‡é€šè¿‡backbone
        x = x.view(B * S, C, L).squeeze(1) # -> [B*S, Samples]
        
        # 2. ä½¿ç”¨ dasheng æå–æ¯ä¸ªç‰‡æ®µçš„æ—¶åºç‰¹å¾
        with torch.set_grad_enabled(not self.freeze_backbone):
            sequence_features = self.backbone(x) 
            segment_embeddings = sequence_features.mean(dim=1) # -> [B*S, Dim]

        # 3. æ¢å¤åºåˆ—ç»´åº¦
        segment_embeddings = segment_embeddings.view(B, S, -1) # -> [B, S, D]
        
        # 4. é€šè¿‡ Transformer Encoder èåˆä¸Šä¸‹æ–‡ä¿¡æ¯
        contextual_embeddings = self.transformer_encoder(segment_embeddings) # -> [B, S, D]
        
        # 5. é€šè¿‡åˆ†ç±»å¤´å¾—åˆ°æ¯ä¸ªç‰‡æ®µçš„ logits
        logits = self.classifier(contextual_embeddings).squeeze(-1) # -> [B, S]
        
        # 6. ä½¿ç”¨ Sigmoid è·å¾—æ¦‚ç‡
        probs = torch.sigmoid(logits)
        
        return probs

# ----------------------------
# è®­ç»ƒä¸éªŒè¯å‡½æ•°
# ----------------------------
def train_one_epoch(model: nn.Module,
                    dataloader: DataLoader,
                    criterion: nn.Module,
                    optimizer: torch.optim.Optimizer,
                    device: torch.device) -> float:
    model.train()
    total_loss = 0.0
    
    bar = tqdm(dataloader, desc="ğŸš€ Training", leave=False)
    for waveforms, labels in bar:
        waveforms = waveforms.to(device)
        labels = labels.to(device)
        
        optimizer.zero_grad()
        
        predictions = model(waveforms)
        
        loss = criterion(predictions.view(-1), labels.view(-1))
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        bar.set_postfix(loss=loss.item())
        
    return total_loss / len(dataloader)


def evaluate(model: nn.Module,
             dataloader: DataLoader,
             criterion: nn.Module,
             device: torch.device):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    bar = tqdm(dataloader, desc="ğŸ§ª Evaluating", leave=False)
    with torch.no_grad():
        for waveforms, labels in bar:
            waveforms = waveforms.to(device)
            labels = labels.to(device)
            
            predictions = model(waveforms)
            
            loss = criterion(predictions.view(-1), labels.view(-1))
            total_loss += loss.item()
            
            binary_preds = (predictions > 0.5).float()
            
            all_preds.append(binary_preds.view(-1).cpu())
            all_labels.append(labels.view(-1).cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()
    
    metrics = {
        "loss": total_loss / len(dataloader),
        "accuracy": accuracy_score(all_labels, all_preds),
        "precision": precision_score(all_labels, all_preds, zero_division=0),
        "recall": recall_score(all_labels, all_preds, zero_division=0),
        "f1": f1_score(all_labels, all_preds, zero_division=0)
    }
    
    return metrics

# ----------------------------
# ä¸»å‡½æ•° (å®Œæ•´ä¿®æ”¹)
# ----------------------------
def main():
    parser = argparse.ArgumentParser(description="ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¹¿å‘Šæ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument('--json_path', type=str, default='./audio_ads.json', help='æ ‡æ³¨ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_dir', type=str, default='./audio/', help='éŸ³é¢‘ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹é‡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=25, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--val_split', type=float, default=0.15, help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--freeze_dasheng', action='store_true', help='æ˜¯å¦å†»ç»“ dasheng éª¨å¹²')
    parser.add_argument('--max_files', type=int, default=-1, help='æœ€å¤§éŸ³é¢‘æ–‡ä»¶æ•° (-1 è¡¨ç¤ºæ‰€æœ‰æ–‡ä»¶)')
    parser.add_argument('--segment_duration', type=float, default=3.0, help='æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--sequence_length', type=int, default=8, help='ä¸Šä¸‹æ–‡çª—å£å¤§å° (ç‰‡æ®µæ•°é‡)')
    parser.add_argument('--output_dir', type=str, default='./output_contextual', help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    # æ–°å¢æ–­ç‚¹æ¢å¤å‚æ•°
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume_epoch', type=int, default=0, help='æ¢å¤è®­ç»ƒçš„èµ·å§‹epoch')
    
    args = parser.parse_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"======= ä½¿ç”¨è®¾å¤‡: {device} =======")
    
    os.makedirs(args.output_dir, exist_ok=True)

    # 1. åŠ è½½å¹¶åˆ†å‰²æ•°æ®é›†ï¼ˆæŒ‰éŸ³é¢‘æ–‡ä»¶çº§åˆ«ï¼‰
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯å¹¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†...")
    with open(args.json_path, 'r') as f:
        all_items = json.load(f)
    
    # å¦‚æœé™åˆ¶äº†æœ€å¤§æ–‡ä»¶æ•°
    if args.max_files > 0:
        all_items = all_items[:args.max_files]
    
    # æŒ‰æ–‡ä»¶åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_items, val_items = train_test_split(
        all_items, 
        test_size=args.val_split, 
        random_state=42  # å›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°
    )
    
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ - æ€»æ–‡ä»¶: {len(all_items)} | è®­ç»ƒæ–‡ä»¶: {len(train_items)} | éªŒè¯æ–‡ä»¶: {len(val_items)}")
    
    # 2. åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†æ•°æ®é›†
    print("ğŸ› ï¸ æ„å»ºè®­ç»ƒæ•°æ®é›†...")
    train_dataset = ContextualAdDataset(
        file_items=train_items,
        audio_dir=args.audio_dir,
        sequence_length=args.sequence_length,
        segment_duration=args.segment_duration
    )
    
    print("ğŸ› ï¸ æ„å»ºéªŒè¯æ•°æ®é›†...")
    val_dataset = ContextualAdDataset(
        file_items=val_items,
        audio_dir=args.audio_dir,
        sequence_length=args.sequence_length,
        segment_duration=args.segment_duration
    )
    
    print(f"   è®­ç»ƒé›†åºåˆ—æ•°: {len(train_dataset)} | éªŒè¯é›†åºåˆ—æ•°: {len(val_dataset)}")

    # 3. åˆ›å»º DataLoader
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨
    print("ğŸ› ï¸ åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹...")
    backbone = dasheng.dasheng_base()
    model = ContextualAdClassifier(
        backbone=backbone,
        freeze_backbone=args.freeze_dasheng
    ).to(device)
    
    criterion = nn.BCELoss()
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=2, verbose=True)
    
    start_epoch = 0
    best_f1 = 0.0
    
    # 5. æ¢å¤è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæä¾›äº†ï¼‰
    if args.resume and os.path.exists(args.resume):
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
        
        # åŠ è½½å®Œæ•´æ£€æŸ¥ç‚¹
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # æ¢å¤è®­ç»ƒçŠ¶æ€
        start_epoch = checkpoint['epoch'] + 1
        best_f1 = checkpoint['best_f1']
        
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–æ£€æŸ¥ç‚¹ä¸­çš„epochè®¾ç½®
        if args.resume_epoch > 0:
            start_epoch = args.resume_epoch
        
        print(f"   æ¢å¤è®­ç»ƒçŠ¶æ€ - èµ·å§‹Epoch: {start_epoch}, æœ€ä½³F1: {best_f1:.4f}")

    # 6. è®­ç»ƒå¾ªç¯ (æ”¯æŒæ–­ç‚¹æ¢å¤)
    for epoch in range(start_epoch, args.num_epochs):
        print(f"\n======== Epoch {epoch+1}/{args.num_epochs} ({(epoch+1)/args.num_epochs*100:.1f}%) ========")
        
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_metrics = evaluate(model, val_loader, criterion, device)
        
        current_f1 = val_metrics['f1']
        
        print(f"\nEpoch {epoch+1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")
        print(f"  {'éªŒè¯å‡†ç¡®ç‡:':<15} {val_metrics['accuracy']:.4f}")
        print(f"  {'éªŒè¯ç²¾ç¡®ç‡:':<15} {val_metrics['precision']:.4f}")
        print(f"  {'éªŒè¯å¬å›ç‡:':<15} {val_metrics['recall']:.4f}")
        print(f"  {'éªŒè¯ F1 åˆ†æ•°:':<15} {val_metrics['f1']:.4f}")
        
        scheduler.step(current_f1)
        
        # 7. åˆ›å»ºæ£€æŸ¥ç‚¹ä¿¡æ¯
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_f1': best_f1,
            'current_f1': current_f1
        }
        
        # 8. ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_checkpoint = os.path.join(args.output_dir, 'checkpoint_latest.pth')
        torch.save(checkpoint, latest_checkpoint)
        print(f"\nğŸ’¾ ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹åˆ°: {latest_checkpoint}")
        
        # 9. ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
        if current_f1 > best_f1:
            best_f1 = current_f1
            best_checkpoint = os.path.join(args.output_dir, 'checkpoint_best.pth')
            torch.save(checkpoint, best_checkpoint)
            
            # å•ç‹¬ä¿å­˜æ¨¡å‹ç”¨äºéƒ¨ç½²
            model_save_path = os.path.join(args.output_dir, 'best_model.pth')
            torch.save(model.state_dict(), model_save_path)
            
            print(f"\nğŸ† æ–°çš„æœ€ä½³ F1 åˆ†æ•°ï¼æ¨¡å‹å·²ä¿å­˜è‡³: {best_checkpoint}")

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ç»ˆæœ€ä½³ F1 åˆ†æ•°ä¸º: {best_f1:.4f}")

if __name__ == '__main__':
    main()